# Rook handbook

## TODOS

TODO:Following options needs to be think!!!!

1. `What will happen when all nodes are rebooting?`
2. CPU-intensive processesï¼Ÿ
3. [Hardware Recommendations](http://docs.ceph.com/docs/master/start/hardware-recommendations/)

## Install Rook

### Bootstrap a rook-ceph-operator

Use `Rook:v.09`

```bash
helm repo add rook-beta https://charts.rook.io/beta

helm install --name rook-ceph --namespace rook-ceph-system --version v0.9.0 rook-beta/rook-ceph
```

```bash
kubectl --namespace rook-ceph-system get pods -l "app=rook-ceph-operator"
```

### Create a Rook Ceph Cluster

#### Ceph Cluster

[See more details](https://rook.github.io/docs/rook/v0.9/ceph-cluster-crd.html)

`WARNING`: For test scenarios, if you delete a cluster and start a new cluster on the same hosts, the path used by `dataDirHostPath(/var/cloud-native/metadata/rook)` must be deleted. Otherwise, stale keys and other config will remain from the previous cluster and the new mons will fail to start. If this value is empty, each pod will get an ephemeral directory to store their config files that is tied to the lifetime of the pod running on that node

```bash
kubectl create -f ./cephcluster.yaml
```

##### Node updates

Add and remove storage resources

```bash
kubectl -n rook-ceph edit cluster.ceph.rook.io rook-ceph
```

### Ceph Block Storage

```bash
kubectl create -f ./storageclass.yaml
```

See OSD Information

```bash
OSD_PODS=$(kubectl get pods --all-namespaces -l \
  app=rook-ceph-osd,rook_cluster=rook-ceph -o jsonpath='{.items[*].metadata.name}')

# Find node and drive associations from OSD pods
for pod in $(echo ${OSD_PODS})
do
 echo "Pod:  ${pod}"
 echo "Node: $(kubectl -n rook-ceph get pod ${pod} -o jsonpath='{.spec.nodeName}')"
 kubectl -n rook-ceph exec ${pod} -- sh -c '\
  for i in /var/lib/rook/osd*; do
    [ -f ${i}/ready ] || continue
    echo -ne "-$(basename ${i}) "
    echo $(lsblk -n -o NAME,SIZE ${i}/block 2> /dev/null || \
    findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type)
  done|sort -V
  echo'
done
```

#### Enable default StorageClass

Enable admission plugins `DefaultStorageClass`, then wait api server to restart

```bash
vim /etc/kubernetes/manifests/kube-apiserver.yaml
# For example
# spec:
#   containers:
#   - command:
#     - kube-apiserver
#     - --authorization-mode=Node,RBAC
#     - --advertise-address=192.168.137.102
#     - --allow-privileged=true
#     - --client-ca-file=/etc/kubernetes/pki/ca.crt
#     - --enable-admission-plugins=NodeRestriction,DefaultStorageClass
```

`IMPORTANT`: For important data, you must change reclaimPolicy to `Retain` from `Delete` manually, beacause rook hasn't implemented pv inherit reclaimPolicy attribute from StorageClass(announced will implemented by rook0.9). Otherwise the reclaimPolicy of every pv generated by `Dynamic Volume Provisioning` will be `Delete`, which is dangerou for important data.

### Ceph Object Storage

TODO: I don't understand the scenarios of ceph object storage
[See more details](https://rook.github.io/docs/rook/v0.9/ceph-object.html)

### Ceph Shared File System

TODO: Need to explore the usage of shared file system
[See more details](https://rook.github.io/docs/rook/v0.9/ceph-filesystem.html)

### About Ceph

[See more details](http://docs.ceph.com/docs/mimic/start/intro/)

### Tear Down

If you want to tear down the cluster and bring up a new one, be aware of the following resources that will need to be cleaned up:

* rook-ceph-system namespace: The Rook operator and agent created by operator.yaml
* rook-ceph namespace: The Rook storage cluster created by cluster.yaml (the cluster CRD)
* `dataDirHostPath(/var/cloud-native/metadata/rook)`: default is /var/lib/rook. Path on each host in the cluster where configuration is cached by the ceph mons and osds

```bash
# If do block storage tutorial
kubectl delete -f ./test/mysql.yaml
kubectl delete -f ./test/wordpress.yaml
kubectl delete -n rook-ceph pool replicapool
kubectl delete storageclass rook-ceph-block

# If do filesystem tutorial
kubectl delete -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/kube-registry.yaml

kubectl -n rook-ceph delete cluster.ceph.rook.io rook-ceph

# Verify that the cluster CRD has been deleted before continuing to the next step.
kubectl -n rook-ceph get cluster.ceph.rook.io

helm delete rook-ceph --purge

kubectl delete namespace rook-ceph

kubectl delete namespace rook-ceph-system

# Delete the data on hosts
# Connect to each machine and delete /var/lib/rook, or the path specified by the dataDirHostPath
rm -rf ${dataDirHostPath}/*
rm -rf /var/cloud-native/metadata/rook/*
```

### Trouble Shooting

[See more detals](https://rook.github.io/docs/rook/v0.8/ceph-teardown.html#troubleshooting)

### Rook Toolbox

[See more deatils](https://rook.github.io/docs/rook/v0.8/toolbox.html)

### Ceph Dashboard

[See more detals](https://rook.github.io/docs/rook/v0.9/ceph-dashboard.html)

```bash
kubectl create -f ./rook-ceph-ingress.yaml
```

```bash
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o yaml | grep "password:" | awk '{print $2}' | base64 --decode
```

### Monitoring

TODO:
[See more details](https://rook.github.io/docs/rook/v0.8/monitoring.html)
